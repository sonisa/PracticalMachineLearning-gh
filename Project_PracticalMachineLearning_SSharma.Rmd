---
title: "Project_PracticalMachineLearning"
author: "Sonisa Sharma"
date: "August 16, 2019"
output:
  html_document:
    fig_height: 4
    fig_width: 4
---

## Background

*The goal of this project is to predict the manner in which the people did the exercise using the devices such as Jawbone Up, Nike Fuel Band, and Fitbit to improve their health, to find patterns in their behaviour*

# Load packages so that it is easy to visualize the data and summarize the data. 

```{r packages}
library(caret)
library(Hmisc)
library(MASS)
library(pgmm)
library(gbm)
library(lubridate)
library(forecast)#install.packages('forecast')
library(elasticnet)#install.packages('elasticnet)
library(e1071)
library(randomForest)
library(mlbench)
```

*The data for this project come from this source: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har. This data will quantify how much of a particular activity they do based on a dataset with 5 classes (sitting-down, standing-up, standing, walking, and sitting) collected on 8 hours of activities of 4 healthy subjects.*


```{r load}
url<- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
download.file(url, destfile="pml-training.csv")
training<- read.csv("pml-training.csv")
url1<- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(url1, destfile="pml-testing.csv")
testing<- read.csv("pml-testing.csv")
```

*There are loads of missing variables so we first clean the data and then divide the data as training datasets as 75% and testing datasets as 35% and then fit the model using different method and see which one is better*


```{r cleaningpartition}
set.seed(125)
#trainin1 = na.omit(training)
#trainData<- training[, colSums(is.na(training)) == 0]
#testData <- testing[, colSums(is.na(testing)) == 0]
index<-which(colSums(is.na(training)|training=="")>0.9*dim(training[1]))
trainin1<-training[,-index]
trainin2<-trainin1[,-c(1:7)]
index1<-which(colSums(is.na(testing)|testing=="")>0.9*dim(testing[1]))
testin1<-testing[,-index1]
testin2<-testing[,-c(1:7)]
inTrain = createDataPartition(trainin2$classe, p = 0.75, list = FALSE)
train = trainin2[inTrain,]
test = trainin2[-inTrain,]
```

# After the data is partitioned, we trained the datasets with different methods as Recursive Partitioning and Regression Trees, classification tree

```{r training}
trControl<- trainControl(method = "cv",number = 5)
modFit <- train(classe ~ ., data = train, method = "rpart",trControl=trControl)
modFit$finalModel
suppressMessages(library(rattle))
library(rpart.plot)
fancyRpartPlot(modFit$finalModel)
```

```{r rpart}
trainpre<-predict(modFit, test)
cm<-confusionMatrix(trainpre,test$classe)
as.table(cm)
eosepre <- 1 - as.numeric(confusionMatrix(test$classe, trainpre)$overall[1])
# plot matrix results
plot(cm$table, col = cm$byClass, 
     main = paste("Decision Tree - Accuracy =", round(cm$overall['Accuracy'], 4)))
```

*This model had overall accuracy of 0.53 with low kappa value and the estimated out-of-sample error is 0.46 (which is high) so all those variables were not sufficient to predict the model so we will use other method too. We used random forests method to see if these method are better than Recursive Partitioning and Regression Trees and classification tree. Random Forest automatically selects important variables. we used 3 fold cross validation to train the algorithm*

```{r, cache = T}
#mod_rf <- rpart(classe ~ ., train)
mod_rf<-randomForest(classe~.,train)
print(mod_rf)
```


Then, we estimate the performance of the model on the validation data set.  
```{r, cache = T}
predict_Rf <- predict(mod_rf, test,type = "class")
cm1<-confusionMatrix(predict_Rf, test$classe)$overall['Accuracy']
cm1
```


```{r, cache = T}
eose <- 1 - as.numeric(confusionMatrix(test$classe, predict_Rf)$overall[1])
eose
```
So, the estimated accuracy of the model is 99.6% and the estimated out-of-sample error is 0.004%. 
These model is better than 'rpart' method. In addition, we will compare with genralized global
boosted regression model 'gbm' to see if these model is better or not.


```{r methods}
mod_gbm <- gbm(classe ~ ., data = train, distribution ="gaussian",cv.folds = 5, n.trees = 200,verbose = FALSE)
print(mod_gbm)$finalModel
```

```{r performance}
# Check performance using 5-fold cross-validation
best.iter <- gbm.perf(mod_gbm, method = "cv")
print(best.iter)
# Check performance using the out-of-bag (OOB) error; the OOB error typically
# underestimates the optimal number of iterations
best.iter <- gbm.perf(mod_gbm, method = "OOB")
print(best.iter)
```

*This model when used found that residual standard error is 0.001. So the 'rf' model is better than'rpart' and 'gbm' model*.

## Applying the best model to the validation data
By comparing the accuracy rate values of the three models, it is clear the the 'rf' model is the winner. 
So will use it on the validation data


```{r result}
Results <- predict(mod_rf, newdata=testin2)
Results
```

## Conclusion

The 'rf' model was the best model found on the three models that we used.

